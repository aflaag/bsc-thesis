\chapter{Finding driver mutations} \label{chap:finding_driver_mutations}

% Although the true explanation for mutual exclusivity remains unknown, and its therapeutic potential is still uncertain, this phenomenon is frequently observed in data and may lead to discoveries in cancer treatment.

In the previous chapter, various studies were discussed in terms of how they formalized biological assumptions, with particular emphasis on the metrics developed to assess \textit{coverage} and \textit{mutual exclusivity} within gene groups. This chapter will delve deeper into the algorithms employed by these studies to identify driver pathways using their respective metrics and hypotheses.

Existing approaches can be categorized into two types: \textbf{\textit{de novo}} approaches, which identify mutually exclusive patterns using only genomic data from patients, and \textbf{\textit{knowledge-based}} methods, which integrate the analysis with external \textit{a priori} information. \textit{De novo} approaches might lack sufficient information as they do not utilize pre-existing pathway databases, protein-protein interaction (PPI) networks or phenotype data. Conversely, given that our understanding of gene and protein interactions in humans is still incomplete, and many pathway databases fail to accurately represent the specific pathways and interactions present in cancer cells, \textit{knowledge-based} approaches may be limited by their dependence on existing data sources. Consequently, \textit{de novo} methods might yield new but potentially less accurate results, while \textit{knowledge-based} approaches may limit the discovery of novel biological insights \cite{survey, multi-dendrix}.

\section{Dendrix}

\subsection{A greedy approach}

\textcite{dendrix} introduced the most widely adopted metric in pathway discovery research, namely $W(M)$ (presented in \cref{weight}). In addition to this, they defined the Maximum Weight Submatrix Problem (MWSP), discussed in \cref{mwsp}, and proposed the following \textit{greedy algorithm}, called Dendrix (\textit{de novo} \cite{survey}), to solve it.

\begin{algorithm}[H]
    \caption{
        \textit{Greedy Dendrix}: given the set of all genes $\mathcal G$, and an integer $k$, the algorithm finds the set of genes $M$ of size $k$ that maximizes $W(M)$.
    }

        \label{greedy_dendrix}
    \begin{algorithmic}[1]
        \Function{greedyDendrix}{$\mathcal G$, $k$}
            \State $M := \{g_1, g_2\}$ such that $M$ maximizes $W(M)$ \Comment{pick the best gene pair}
            \For{$i \in [3, k]$}
                \State $\hat g := \argmax_{g \in \mathcal G}{W \rbk{M \cup \{g\}}}$ \Comment{TODO \href{http://web.archive.org/web/20220805140851/https://compbio.cs.brown.edu/software/}{SOURCE} PER PARIM?}
                \State $M = M \cup \{\hat g\}$
            \EndFor
            \State \textbf{return} $M$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

Clearly, the time complexity of the algorithm is $O\rbk{n^2 + kn} = O\rbk{n^2}$ --- where $n = \abs{\mathcal G}$, therefore $k \le n$ --- because finding $\{g_1, g_2\}$ in line 2 requires $O\rbk{n^2}$ and the $\argmax{}$ in line 4 has cost $O(n)$. 

While this algorithm is efficient, there is generally no guarantee that it will identify the optimal set $\hat M$ that maximizes $W(\hat M)$. However, \textcite{dendrix} prove that \cref{greedy_dendrix} can correctly identify $\hat M$ with high probability when the mutation data come from the \textit{Gene Independence Model} (GIM), which is described below. \todo{ne forniscono una dimostrazione nel materiale supplementare, ma è lunga 3 pagine, e non credo sia oppurtuno inserire una cosa cosi lunga qui, penso che sia \curlyquotes{beyond the scope}}

\begin{definition}[Gene Independence Model] \todo{capire bene questo modello}
    Let $A$ be an $m \times n$ mutation matrix, such that $\hat M$ is the \textit{maximum weight submatrix} of $A$, and $|\hat M| = k$; the matrix $A$ satisfies the \textbf{Gene Independence Model} (GIM) if and only if:

    \begin{enumerate}[label=\roman*), font=\itshape]
        \item each gene $g \notin \hat M$ is mutated in each patient with probability $p_g \in \sbk{p_L, p_U}$ \todo{WHAT ARE THESE??}, independently of all other events;
        \item $W (\hat M)= \Omega(m)$; \todo{qua scrivono che la definizione di $Omega$ è che $W (\hat M) = rm$ per $0 < r \le 1$???????}
        \item for all $M \subset \hat M$ of cardinality $l := \abs{M}$, it exists $0 \le d < 1$ such that $$W(M) \le \frac{l + d}{k} W(\hat M)$$
    \end{enumerate}
\end{definition}

Note that:

\begin{itemize}
    \item condition $(i)$ reflects the independence of mutations for genes outside the mutated pathway, a standard assumption for somatic single-nucleotide mutations, according to \textcite{dendrix};
    \item condition $(ii)$ ensures that mutations in $\hat M$ cover a large number of patients and are mostly exclusive;
    \item condition $(iii)$ means that each gene in $\hat M$ is important, so there are no subset of $\hat M$ that predominantly contributes to $W(\hat M)$.
\end{itemize}

Although it is possible to efficiently obtain accurate results with high probability under the GIM, genes within $\hat M$ may exhibit \textit{observed mutation frequencies} similar to those of genes outside $\hat M$. This similarity can make it challenging to distinguish between them based solely on mutation frequency, regardless of the number of patients.

To illustrate this, consider the following scenario: observed gene mutation frequencies fall within the range of $[3 \times 10^{-5}, 0.13]$ --- based on a background mutation rate of $\approx 10^{-6}$ \cite{tcga}. If somatic mutations are measured in $n = 20,000$ human genes, and the size of $\hat M$ is 10, then approximately 2,400 patients are needed for the greedy algorithm to identify $\hat M$ with a probability of at least $1 - 10^{-4}$. While this number of patients is expected to be available from large-scale cancer sequencing projects \cite{icgc}, it exceeds current availability.

Therefore, in practical applications, the effectivness of this greedy algorithm depends on having mutation data from a sufficiently large number of patients. Moreover, the GIM model may be appropriate for certain types of somatic mutations, such as \href{https://en.wikipedia.org/wiki/Single-nucleotide_polymorphism}{single-nucleotide aberrations}, but may not be suitable for others. To address these limitations, \textcite{dendrix} developed an alternative approach, which will be detailed in the following section.

\subsection{Using MCMC}

To overcome the drawbacks of the aforementioned greedy algorithm, \textcite{dendrix} developed a \href{https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo}{\textit{Markov Chain Monte Carlo}} (MCMC) approach, which does not rely on assumptions about the distribution of mutation data or the number of patients. In particular, this MCMC method does not assume independence among mutations in different genes, making it particularly useful for analyzing copy-number aberrations (CNAs), which often involve correlated mutations due to amplification or deletion of adjacent genes.

\textcite{dendrix} employed a \href{https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm}{Metropolis-Hastings} algorithm to sample sets $M \subseteq \mathcal G$ of $k$ genes, with the following stationary distribution, proportional to $e^{cW(M)}$, for some constant $c > 0$ $$\pi(M) = \dfrac{e^{cW(M)}}{\sum_{R \in \mathcal M_k}{e^{cW(R)}}}$$ where $\mathcal M_k := \{M \subset \mathcal G : \abs{M} = k\}$. While there are no guarantees on the rate of convergence of the Metropolis-Hasting algorithm to the stationary distribution, \textcite{dendrix} prove that their MCMC is rapidly mixing, therefore the stationary distribution is effectively reached in a practical number of steps.

The main idea of the MCMC algorithm involves constructing a \href{https://en.wikipedia.org/wiki/Markov_chain}{Markov chain}, where each state represents a collection of $k$ columns from a given mutation matrix $A$, and transitions between these states occur by swapping one gene. Further details of the algorithm are discussed below.

\begin{definition}[Dendrix's MCMC]
    The \textbf{MCMC's procedure} of Dendrix is defined through the following steps:

    \begin{enumerate}
        \item \textit{initialization}: given the set of all genes $\mathcal G$, choose an arbitrary subset $M_0 \subseteq \mathcal G$ of $k$ genes;
        \item \textit{iteration}: for $t = 1, 2, \ldots$ derive $M_{t + 1}$ from $M_t$ as follows:
            \begin{enumerate}
                \item choose a gene $w$ uniformly, at random, from $\mathcal G$;
                \item choose a gene $v$ uniformly, at random, from $M_t$;
                \item let $M_t' := (M_t - \{v\}) \cup \{w\}$;
                \item let $\Delta_W := W\rbk{M_t'} - W(M_t)$;
                \item let $P(M_t, w, v) := \min \sbk{1, e^{c \Delta_W}}$, where $c > 0$;
                \item set $M_{t+1} := M_t'$ with probability $P(M_t, w, v)$, else $M_{t + 1} := M_t$.
            \end{enumerate}
    \end{enumerate}
\end{definition}

Note that:

\begin{itemize}
    \item $w$ is choosen from $\mathcal G$, thus if $w \in M_t$ then $$M_t' = (M_t - \{v\}) \cup \{w\} = M_t - \{v\}$$ which means that no genes were added to $W_t'$; this must be allowed because maximizing the weight may require removing genes already present in the current set, without adding new ones;
    \item $\Delta_W$ measures the change in the weight function with the new set, and the constant $c$ is a scaling factor that adjusts the importance of this difference; note that $$\Delta_W \ge 0 \implies e^{c \Delta_W} \ge 1 \implies P(M_t, w, v) = 1 \implies M_{t + 1} = M_t'$$ in fact when $\Delta_W > 0$ the weight has improved, therefore the next iteration should start from $M_t'$; conversely $$\Delta_W < 0 \implies e^{c \Delta_W} < 1 \implies P(M_t, w, v) = e ^{c \Delta_W}$$ which means that when $\Delta_W < 0$ (i.e., the weight has decreased) the change will be perfomed with probability $e^{c \Delta_W}$ --- note that this value is still close to 1 if the weight has not decreased significantly.
\end{itemize}

\textcite{dendrix} prove that their MCMC is rapidly mixing for some $c > 0$, but details of this proof are beyond the scope of this work. The following sections will briefly discuss some of the results obtained with their algorithm.

\subsection{Results}

This section will briefly discuss the results reported by \textcite{dendrix} from running the MCMC algorithm on real data. To improve the efficiency of the algorithm, the mutation matrix was optimized by merging genes $T = \{g_1, \ldots, g_h\}$ that were mutated in the same patients into larger \textit{metagenes} $g_T$, where each metagene represents the combined mutations occurring in those same patients. The MCMC algorithm samples gene sets with a frequency proportional to their weights, thus in order to focus on high-weight sets, only those with a frequency of at least 1\% were reported.

\textcite{dendrix} applied their MCMC algorithm (with the constant $c$ set to $c = 0.5$) to analyze somatic mutations obtained from high-throughput genotyping of 238 oncogenes across 1,000 patients, spanning 17 cancer types (a study conducted by \textcite{thomas}). A mutation matrix was constructed with 298 patients and 18 mutation groups, based on the groupings from \textcite{thomas}. They ran the MCMC algorithm on gene sets ranging in size from 2 to 10, sampling every 10,000 iterations after running the algorithm for 10 million iterations. The analysis identified a set of 8 mutation groups, altered in 94\% of patients with at least one mutation, totaling 295 mutations ($p < 0.01$). They state that these mutated genes are linked to well-known cancer pathways. Additionally, two sets of size 10, which included the initial 8 mutation groups, were found in 95\% of patients, accounting for 302 mutations ($p < 0.01$).

They also applied their algorithm to somatic mutations in \href{https://en.wikipedia.org/wiki/Adenocarcinoma_of_the_lung}{lung adenocarcinoma}, using data from The Cancer Genome Atlas (TCGA) \cite{tcga}, which included 188 patients and 623 genes, of which 356 were found to be mutated in at least one patient. For gene sets of size $k = 2$, the pair (EGFR, KRAS) was identified in 99\% of samples, covering 90 patients, with no overlap, indicating high mutual exclusivity; additionally, when analyzing sets of size $k = 3$, the algorithm uncovered a novel triplet (EGFR, KRAS, STK11), appearing in 8.4\% of samples; the significance of both the pair and the triplet was confirmed through permutation tests. \textcite{dendrix} highlight that all three genes are involved in regulating the \href{https://en.wikipedia.org/wiki/MTOR}{mTOR} pathway, which is known to be crucial in lung adenocarcinoma. To identify additional sets, the MCMC algorithm was rerun after removing the triplet, revealing the pair (ATM, TP53) with a sampling frequency of 56\%, covering 76 patients. Although these reported sets showed high exclusivity, their relatively low coverage suggests that they may not represent complete driver pathways. Indeed, \textcite{dendrix} propose that this could be due to the limited number of genes analyzed or the focus on specific mutation types. Moreover, there was no significant overlap between patients with mutations in (ATM, TP53) and those with mutations in (EGFR, KRAS, STK11), suggesting that these gene sets likely belong to distinct biological pathways.

The MCMC algorithm was also applied to mutation data from 84 patients with \href{https://en.wikipedia.org/wiki/Glioblastoma}{glioblastoma multiforme} (GBM), analyzing somatic mutations across 601 genes, resulting in a total of 453 mutations, with 223 genes found to be mutated in at least one patient (filtered using CNAs). For gene sets of size $k = 2$, the most frequently sampled pair was (CDKN2B, CYP27B1), appearing 18\% of the time, and for $k = 3$, the triplet (CDKN2B, RB1, CYP27B1) was sampled 10\% of the time --- a permutation test confirmed the significance of this triplet. The analysis revealed that CYP27B1 had a nearly identical mutation profile to a metagene composed of six adjacent genes, but was excluded due to an additional mutation in a single patient. The metagene's amplification likely targeted CDK4, suggesting that the key triplet of interest was (CDKN2B, RB1, CDK4), which is part of the \href{https://en.wikipedia.org/wiki/Retinoblastoma_protein}{RB1} signaling pathway, associated with shorter survival in GBM patients. After removing this triplet, the pair (TP53, CDKN2A) was sampled with 30\% frequency, linked to the \href{https://en.wikipedia.org/wiki/P53}{p53} tumor suppression pathway. Further analysis, after removing both sets, revealed the pair (NF1, EGFR), which was sampled 44\% of the time and is part of the \href{https://en.wikipedia.org/wiki/Receptor_tyrosine_kinase}{RTK} pathway, crucial for cell proliferation and survival.

All these findings highlight the ability of the MCMC algorithm to identify significant gene sets related to known cancer pathways. The following section will introduce an alternative approach to solving the MWSP, using the same weight function.

\section{MDPFinder}

\subsection{The genetic algorithm}

placeholder. \todo{sezione in cui scrivo che mpdfinder critica la soluzione esatta trovata da multi-dendrix, che nonostante sia esatta potrebbe non essere reale; in questa sezione va menzionato che l'algoritmo si chiama MDPFinder, e va scritto che è knowledge-based}

The approach used by \textcite{mdpfinder} involves a \textit{Genetic Algorithm} (GA), which is versatile and flexible, and can be used to optimize a wide variety of scoring functions. In addition, they state that the GA approach is particularly relevant to the current problem due to its conceptual alignment with the notions of \textit{gene} and \textit{mutation}. It models genetic variation within a population, evolving through a process of random selection, thereby avoiding the need to enumerate all possible solutions.

\begin{definition}[Hypothesis space]
    A \textbf{member} of the population is defined by a binary string of length $n$, i.e. the number of genes. Given a gene set $M$, the value of the $i$-th position of an individual represents the membership of the $i$-th gene in $M$.

    Therefore, the \textbf{hypothesis space} is constituted by all the possible binary strings with length $n$ that have $k$ 1s, namely $$\mathcal H := \left\{(x_1, \ldots, x_n) \mid x_i \in \{0, 1\}, i \in [1, n], \sum_{j = 1}^n {x_j} = k \right\}$$
\end{definition}

\begin{definition}[Fitness function]
    The \textbf{fitness} $f_i$ of each individual $h_i$ (its corresponding gene set is $M_i$) of the population is defined as the rank $r_i$ of the score $W(M_i)$, in the ascending order \todo{SECONDO ME È DESCENDING ALTRIMENTI PRENDI QUELLO COL PESO MINORE CHE NON HA SENSO, probabilmente non sto capendo il senso della frase}: $$\forall h_i \in \mathcal H \quad f_i := r_i$$
\end{definition}

\begin{definition}[Selection probability] \label{selection_probability}
    Given the rank $r_i$ of an individual $h_i$ based on its score, the \textbf{selection probability} is defined as follows: $$p_i = \dfrac{2r_i}{P(P + 1)}$$ where $P$ is the population size. The individual with the highest fitness value is most likely to be transferred into the next generation.
\end{definition}

This selection operator is based on \href{https://en.wikipedia.org/wiki/Selection_(genetic_algorithm)#Roulette_wheel_selection}{roulette wheel selection}, which states that the probability of choosing an individual is equal to $$p_i = \dfrac{f_i}{\sum_{j = 1}^P {f_j}} = \dfrac{r_i}{\frac{P(P + 1)}{2}} = \dfrac{2r_i}{P(P+1)}$$ which is precisely the equation in \cref{selection_probability}.

\begin{definition}[Crossover operator]
    The \textbf{crossover operator} specifies the breeding process as follows: the offspring inherits the variables shared by both parents, while the non-shared ones are selected from the symmetric difference of the parents' genetic makeup.
\end{definition}

\begin{definition}[Mutation operator]
    The \textbf{mutation operator} randomly sets the value of one variable from 1 to 0, and changes another variable value from 0 to 1, ensuring the feasibility of every offspring.
\end{definition}

\begin{definition}[Local search] \todo{fixa questa definizione}
    To prevent premature convergence and enhance the accuracy of the algorithm, a local search strategy is employed to improve search performance. In particular, the values of two variables are randomly altered, as the mutation operator. If this adjustment improves the current solution, it is accepted; the search is terminated once all variables have been tested with this routine.
\end{definition}

\begin{definition}[GA procedure]
    The following are the details of the \textbf{GA procedure}:

    \begin{enumerate}
        \item \textit{population generation}: a random population of size $P$ and mutation rate $p_m$ is generated, where $P = n$ (i.e. the number of available genes);
        \item \textit{breeding}: for each iteration, $P$ couples are selected from the current population, based on $p_i$, and each couple generates an offspring;
        \item \textit{mutation}: each offspring may optionally receive a mutation with probability $p_m$;
        \item \textit{selection}: all parents and offspring are ranked based on their scoring values, and the top $P$ individuals are selected to form the next generation (this is referred to as \href{https://en.wikipedia.org/wiki/Truncation_selection}{truncation selection});
        \item \textit{local search}: verify if the iteration is stuck in a local solution (e.g. if the maximum scoring value does not improve over two consecutive iterations); if this is the case, perform a local search;
        \item \textit{termination}: proceed as such until the termination criterion is met (e.g. if the current maximum scoring value does not improve over 10 consecutive iterations); if this occurs, then end the procedure.
    \end{enumerate}
\end{definition}

\subsection{The integration procedure}

In practical applications, multiple optimal solutions may exist. Additionally, due to data noise and other factors, the solutions considered optimal --- i.e. the ones with the highest $W(M)$ --- may not necessarily be the most relevant in a biological context. To identify the most biologically meaningful solutions, other types of data are integrated to refine the results. Specifically, the GA procedure is extended by incorporating gene expression data to enhance its performance. The integrative model is developed based on the observation that genes within the same pathway typically collaborate to perform a specific function. Consequently, the expression profiles of gene pairs within the same pathway often exhibit higher correlations than those in different pathways. This characteristic can be leveraged to distinguish between gene sets that have the same score: the model focuses on detecting gene sets whose scores $W(M)$ are close to the optimal solution, but whose member genes display stronger correlations with each other.

\begin{definition}[Integrative measure]
    Given an $m \times n$ mutation matrix $A$, an expression matrix $E$ with the same dimensions \todo{nel paper non è menzionato cosa questa "expression matrix" contenga all'interno, c'è una foto con dei colori e basta, però ho visto che "expression matrix" è un termine che si usa per indicare un tipo di matrici solamente che contengono una cosa ben specifica, posso assumere che si sappia già di cosa si parla o devo spiegare cosa sono?}, and an $A$'s submatrix $M$ of size $m \times k$, the integrative model is defined by the following \textbf{measure}: $$F_{ME} := W(M) + \lambda \cdot R(E_M)$$ where $E_M$ is $E$'s expression submatrix that corresponds to $M$, and $R(E_M)$ is described by the following equation: $$R(E_M) = \sum_{j_1 = 1}^n {\sum_{j_2 = 1 \atop j_1 \neq j_2} ^n {\dfrac{\abs{\mathrm{pcc}(x_{j_1}, x_{j_2})}}{\frac{k(k - 1)}{2}}}}$$ where $\mathrm{pcc}(\cdot)$ is the \href{https://en.wikipedia.org/wiki/Pearson_correlation_coefficient}{Pearson correlation coefficient}, and $x_j$ is the expression profile of gene $j$ \todo{non è spiegato cosa questo voglia dire ma io assumo sia un vettore colonna di $E$; inoltre, non ho idea di cosa sia il valore $R(E_M)$}.
\end{definition}

Note that $$0 \le R(E_m) \le 1$$ and $W(M)$ is an integer, therefore when $\lambda = 1$ the value of $F_{ME}$ can be used to discriminate the gene sets with the same $W(M)$. Moreover, for values of $\lambda \ge 1$, the gene set with strong correlation and approximate exclusivity can be identified.

\section{Mutex}

\subsection{A different greedy method}

To identify the most mutually exclusive group, \textcite{mutex} employ a greedy algorithm called Mutex (\textit{knowledge-based} \cite{survey}), which is applied to a directed graph constructed from databases containing information about biological pathways \todo{menziono i database che hanno usato?}.

The search begins by initializing a set with an altered gene as the seed and then expanding the group greedily with the next best candidate gene. Candidate genes are selected such that, after their addition, the group still has a common downstream gene that can be accessed without passing through any non-member genes (the common downstream gene may also be a member of the group) \todo{ci sono delle figure con un grafo e l'insieme che viene proressivamente espanso dall'algoritmo, forse potrei riprodurle e inserirle per chiarezza?}. The group is expanded with the candidate that improves the group score the most. The process continues until no candidates remain or the group reaches a preset size threshold. The algorithm outputs a group and its score for each seed gene \todo{qui menzionano una cosa inerente al controllo dell'FDR ma per ora la ometto perché non so che significa, credo andrebbe inserita}.

placeholder. \todo{una cosa carina sarebbe scrivere lo pseudocodice dell'algoritmo greedy (sarebbe anche più carino a quel punto dimostrarne la correttezza ma non credo sia possibile vista la natura statistica della ricerca effettuata), loro non lo forniscono e lo trattano solo a parole, provo a farlo?}

\begin{definition}[Proximity]
    Given a gene graph, the \textbf{proximity} of a gene $G$ includes not only the genes directly adjacent to $g$ but also those that share downstream targets in the pathway with $g$. \todo{QUESTA DEFINIZIONE NON VA QUA}
\end{definition}

\section{Multi-Dendrix} \todo{MOVE THIS DOWN}

\subsection{An alternative approach to the MWSP}

\textcite{dendrix} propose a solution to the MWSP that may not appear immediately intuitive, given that the problem itself resembles an \href{https://en.wikipedia.org/wiki/Optimization_problem}{optimization problem}. Indeed, \textcite{multi-dendrix}, the authors of Multi-Dendrix (\textit{de novo} \cite{survey}), present an alternative approach by formulating the problem as an \textit{Integer Linear Program} (ILP), called $\mathrm{Dendrix}_{\mathrm{\textit{ILP}}}(k)$, which is described below. 

To begin, it is necessary to define two sets of indicator variables: consider a gene set $M$, described by a set of variables, one for each gene $j \in M$, defined as follows

\begin{equation}
    I_M(j) = 1 \iff j \in M
\end{equation}

and a set of indicator variables, one for each patient $i$, expressed in this form

\begin{equation} \label{c_idefn}
    C_i(M) = 1 \iff \exists g \in M \mid i \in \Gamma(g)
\end{equation}

therefore $C_i(M)$ is equal to 1 if and only if $M$ \textit{covers} the $i$-th patient.

The ILP formulation provided by \textcite{multi-dendrix} is illustrated below.

\begin{definition}[$\mathrm{Dendrix}_{\mathrm{\textit{ILP}}}(k)$]
    $\mathrm{Dendrix}_{\mathrm{\textit{ILP}}}(k)$ is defined by the following ILP:

    \begin{equation} \label{weight_dendrix_ilp}
        \mathrm{maximize} \sum_{i = 1}^m {\rbk{2 \cdot C_i(M) - \sum_{j = 1}^n I_M(j) \cdot a_{i, j}}},
    \end{equation}

    \begin{equation} \label{second_constr_dendrix_ilp}
        \mathrm{subject \ to} \sum_{j = 1}^n{I_M(j) = k},
    \end{equation}

    \begin{equation} \label{third_constr_dendrix_ilp}
        \sum_{j = 1}^n I_M(j) \cdot {a_{i, j}} \ge C_i(M),
    \end{equation}

    \begin{equation*}
        \mathrm{for\ } 1 \le i \le m.
    \end{equation*}
\end{definition}

Note that \cref{weight_dendrix_ilp} uses the second version of the definition provided in \cref{weight}, and \cref{second_constr_dendrix_ilp} limits the size of $M$ to be exactly $k$. Moreover, note that \cref{third_constr_dendrix_ilp} only forces $C_i(M) = 0$ when the $i$-th patient has no mutated genes in $M$, but does not force $C_i(M) = 1$ when the patient has at least one, as required by \cref{c_idefn}. However, the objective function will be maximized when $C_i(M)=1$, thus \cref{c_idefn} is satisfied.

\begin{lemma}[Correctness of $\mathrm{Dendrix}_{\mathrm{\textit{ILP}}}(k)$] Given a gene set $M$, the sum in \cref{weight_dendrix_ilp} correctly evaluates $W(M)$.
\end{lemma}

\begin{proof}
    Rearranging the terms in \cref{weight_dendrix_ilp} $$\sum_{i = 1}^m {\rbk{2 \cdot C_i(M) - \sum_{j = 1}^n I_M(j) \cdot a_{i, j}}} = 2\sum_{i = 1}^m {C_i(M)} - \sum_{i = 1}^m {\sum_{j = 1}^n {I_M(j) \cdot a_{i, j}}}$$ and it is trivial to check that $$\abs{\Gamma(M)} = \sum_{i = 1}^m {C_i(M)}$$ since it it true by definition, and $$\sum_{g \in M}{\abs{\Gamma(g)}} = \sum_{i = 1}^m {\sum_{j = 1}^n {I_M(j) \cdot a_{i, j}}}$$ because the RHS counts the number of cells of $A$ such that $a_{i, j} = 1$ for every $j \in M$.
\end{proof}

The next section will discuss how \textcite{multi-dendrix} extended this ILP formulation to enable the search for multiple driver pathways.

\subsection{The ILP}

As outlined in \cref{multi_dendrix_2nd_chap}, \textcite{multi-dendrix} propose that the most effective approach for this research is to identify multiple driver pathways rather than focusing on a single one. To accomplish this, they extended the weight metric introduced by \textcite{dendrix} to find a collection of gene sets that maximizes the sum of their individual weights. Specifically, they extended the MWSP as follows:

\begin{displayquote}
    \textbf{Multiple Maximum Weight Submatrices Problem} (MMWSP): Given an $m \times n$ mutation matrix $A$, and integer $t > 0$, and two integers $k_{\mathrm{min}}, k_{\mathrm{max}} \ge 0$, find a collection $M = \{M_1, \ldots, M_t\}$ of column submatrices that maximizes $W'(M)$, where each submatrix $M_\rho$ --- for $1 \le \rho \le t$ ---  has size $m \times k_\rho$ for some $k_{\mathrm{min}} \le k_\rho \le k_{\mathrm{max}}$.
\end{displayquote}

Note that the sets in the optimal collection may vary in size, as different pathways are likely to have different lengths; additionally, note that this problem is NP-Hard, as for the case where $t = 1$ (proof provided in \cref{mwsp proof}) \todo{fix this probably}. Furthermore, \textcite{multi-dendrix} state that collections $M$ with a large value of $W'(M)$ are also likely to exhibit higher coverage $\Gamma(M_\rho)$, for each individual gene set $M_\rho$. Consequently, optimal solutions tend to produce collections where many patients have mutations in more than one gene set, which may involve pairs or larger groups of co-occurring mutations --- a phenomenon observed in real cancer data.

The ILP developed by \textcite{multi-dendrix} to simultaneously search for multiple driver pathways is described below.

\begin{definition}[Multi-Dendrix]
    Multi-Dendrix is defined by the following ILP:

    \begin{equation} \label{weight_multi-dendrix_ilp}
        \mathrm{maximize} \sum_{\rho = 1}^t {\sum_{i = 1}^m {\rbk{2 \cdot C_i(M_\rho) - \sum_{j = 1}^n I_{M_\rho}(j) \cdot a_{i, j}}}},
    \end{equation}

    \begin{equation} \label{second_constr_multi-dendrix_ilp}
        \mathrm{subject \ to} \sum_{j = 1}^n I_{M_\rho}(j) \cdot {a_{i, j}} \ge C_i(M_\rho),
    \end{equation}

    \begin{equation} \label{multi-dendrix_ilp_size}
        k_\mathrm{min} \le \sum_{j = 1}^n {I_{M_\rho}(j)} \le k_\mathrm{max},
    \end{equation}
    
    \begin{equation*}
        \mathrm{for\ } 1 \le i \le m, \ 1 \le \rho \le t,
    \end{equation*}

    \begin{equation} \label{multi-dendrix_ilp_last}
        \sum_{\rho = 1}^t{I_{M_\rho}(j)} \le 1, \ 1 \le j \le n.
    \end{equation}
\end{definition}

Note that:

\begin{itemize}
    \item \cref{weight_multi-dendrix_ilp} and \cref{second_constr_multi-dendrix_ilp} extend \cref{weight_dendrix_ilp} and \cref{second_constr_dendrix_ilp} respectively;
    \item \cref{multi-dendrix_ilp_size} allows each gene group to have a size between $k_\mathrm{min}$ and $k_\mathrm{max}$;
    \item \cref{multi-dendrix_ilp_last} forces each gene to appear in \textit{at most 1 set} within the collection. 
\end{itemize}

Moreover, \textcite{multi-dendrix} state that this ILP can be extended to allow the gene sets of the collection to overlap, since the genes in the intersection may be involved in multiple biological processes. Hence, \cref{multi-dendrix_ilp_last} is replaced with the following equation:

\begin{equation}
    \sum_{\rho = 1}^t {I_{M_\rho}(j) \le \Delta}, \quad 1 \le j \le n
\end{equation}

where $\Delta$ is the maximum number of gene sets a gene can be a member of, and the following constraint is added:

\begin{equation}
    \sum_{j = 1}^n {\sum_{\rho = 1}^t \sum_{\rho' = 1 \atop \rho \neq \rho'}^t {{I_{M_\rho} (j) \cdot I_{M_{\rho'}} (j)}}} \le \tau, \quad 1 \le \rho \le t
\end{equation}

where $\tau$ is the maximum size of the intersection between two gene sets.


\subsection{Comparing Multi-Dendrix with Iter-Dendrix}

Since the greedy algorithm of Dendrix can identify a single driver pathway, finding multiple pathways could be achieved by running the algorithm iteratively. \textcite{multi-dendrix} provide a detailed explanation of this approach, referred to as Iter-Dendrix, with the pseudocode shown below.

\begin{algorithm}[H]
    \caption{
        \textit{Iter-Dendrix}: given the set of all genes $\mathcal G$, an integer $k$, and an integer $t$, the algorithm finds the collection $M$ of $t$ gene sets of size $k$ that maximizes $W'(M)$.
    }

        \label{iter-dendrix}
    \begin{algorithmic}[1]
        \Function{iterDendrix}{$\mathcal G$, $k$, $t$}
            \State $M := \varnothing$
            \For{$i \in [1, t]$}
            \State $M_i := \texttt{greedyDendrix}(\mathcal G, k)$ \Comment{procedure defined in \cref{greedy_dendrix}}
                \State $M = M \cup \{M_i\}$
                \State $\mathcal G = \mathcal G - M_i$
            \EndFor
            \State \textbf{return} $M$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

This procedure runs the greedy algorithm iteratively, removing the chosen set from $\mathcal{G}$ after each iteration. \textcite{dendrix} discussed this approach toward the end of their work, highlighting certain limitations. In particular, if the gene sets corresponding to each pathway are disjoint, Iter-Dendrix can be effective in identifying these sets, successfully finding disjoint sets $M_1$ and $M_2$ with high weight, as exclusivity is only evaluated within sets, not between them. However, if $M_1$ and $M_2$ share genes, removing one set could also remove part of the other. In cases where the overlap is minimal, this approach may still identify the remaining portion of the second set. However, if the sets significantly intersect, Iter-Dendrix is likely to fail \cite{dendrix}.

\textcite{multi-dendrix} compare the outputs of their ILP with Iter-Dendrix: denoting with $M$ and $I$ the collections of gene sets obtained from Multi-Dendrix and Iter-Dendrix respectively, they state that $W'(M) \ge W'(I)$ \todo{loro dicono che sia ovvio ma non capisco perché dovrebbe essere così ovvio}. They also argue that $M$ could contain sets with strictly greater weight than the ones comprising $I$, due to several factors:

\begin{itemize}
    \item there may be multiple gene sets $I_\rho$ that maximize $W(I_\rho)$ on the $\rho$-th iteration of Iter-Dendrix, and this version of Dendrix can only extend one of them;
    \item the gene set $I_\rho$ that maximizes $W(I_\rho)$ selected by Iter-Dendrix in the $\rho$-th iteration may not be a member of $M$, since $M$ could include gene sets that are suboptimal when considered in isolation;
    \item when $k_\mathrm{min} < k_\mathrm{max}$, Multi-Dendrix may choose gene sets with fewer than $k_\mathrm{max}$ genes, if doing so maximizes the overall weight $W'(M)$.
\end{itemize}

\textcite{multi-dendrix} state that all of these scenarios occur when analyzing real mutation data.

\subsection{Results}

\textcite{multi-dendrix} applied Multi-Dendrix and Iter-Dendrix to four somatic mutation datasets: GBM, lung adenocarcinoma, a newer GBM dataset, and BRCA. These datasets were processed to remove low-frequency mutations and outliers. After processing, the GBM dataset included 46 genes from 84 patients, the lung dataset had 190 genes from 163 patients, the newer GBM dataset contained 398 genes from 261 patients, and the BRCA dataset included 375 genes from 507 patients. They focused on results from the GBM and BRCA datasets, as they are more representative of modern genomic data, obtained from computing collections of sizes ranging between $2 \le t \le 4$, with a minimum size $k_\mathrm {min} = 3$, and a maximum size ranging between $3 \le k_\mathrm{max} \le 5$.

In the GBM analysis, both algorithms produced similar results, except Iter-Dendrix identified the \href{https://en.wikipedia.org/wiki/IRF5}{IRF5} gene in one case, though Multi-Dendrix ran significantly faster (142 seconds compared to \textit{over 10 hours}). They identified four main modules in the data, corresponding to key signaling pathways related to cancer, with mutations affecting a large proportion of samples:

\begin{itemize}
    \item RB signaling pathway: this module, including genes such as \href{https://en.wikipedia.org/wiki/Cyclin-dependent_kinase_4}{CDK4}, RB1 and \href{https://en.wikipedia.org/wiki/CDKN2A}{CDKN2A}/\href{https://en.wikipedia.org/wiki/CDKN2B}{B}, was mutated in 87.7\% of samples, and it also included mutations in \href{https://www.genecards.org/cgi-bin/carddisp.pl?gene=MSL3}{MSL3}, a gene with a potential role in cancer that merits further investigation;
    \item RTK/RAS/PI(3)K pathway: this module included \href{https://en.wikipedia.org/wiki/PTEN_(gene)}{PTEN}, \href{https://en.wikipedia.org/wiki/P110%CE%B1}{PIK3CA}, \href{https://en.wikipedia.org/wiki/PIK3R1}{PIK3R1}, and \href{https://en.wikipedia.org/wiki/IDH1}{IDH1}, among others; mutations in this module were present in 62.8\% of samples, and while IDH1 is not a known member of this pathway, its mutual exclusivity with other genes suggests complex interactions;
    \item p53 signaling pathway: this module featured TP53, \href{https://en.wikipedia.org/wiki/Mdm2}{MDM2}, \href{https://en.wikipedia.org/wiki/MDM4}{MDM4}, and \href{https://en.wikipedia.org/wiki/NLRP3}{NLRP3}, affecting 57.8\% of samples; it highlights critical interactions in cancer progression and includes NPAS3, which has emerging links to GBM.
    \item RTK/RAS/PI(3)K and RB pathways: this module, involving \href{https://en.wikipedia.org/wiki/Epidermal_growth_factor_receptor}{EGFR}, \href{https://en.wikipedia.org/wiki/Platelet-derived_growth_factor_receptor_A}{PDGFRA}, and RB1, appeared in 45.6\% of samples; while EGFR and PDGFRA are part of the RTK/RAS/PI(3)K pathway and RB1 is in the RB pathway, the mutual exclusivity here may be influenced by subtype-specific mutations;
\end{itemize}

When applying the two algorithms to the BRCA dataset, at first the algorithms grouped frequently mutated genes into single sets despite their high coverage overlap. This was due to the weight function outweghing coverage $\abs{\Gamma(M)}$ over overlap $\omega(M)$. To enhance mutual exclusivity, they increased the overlap penalty, by using the following modifed weight function: $$W(M) = \abs{\Gamma(M)} - \alpha \omega(M)$$ and a value of $\alpha = 2.5$. With this adjustment, Multi-Dendrix identified four distinct modules:

\begin{itemize}
    \item PI(3)K/AKT pathway: this module contains genes such as PTEN, PIK3CA, PIK3R1, \href{https://en.wikipedia.org/wiki/AKT1}{AKT1}, and \href{https://en.wikipedia.org/wiki/HIF3A}{HIF3A}, and an amplification at 12p13.33; it is mutated in 61\% of samples, and it includes not only key genes in this pathway, but also the 12p amplification, though its target remains unclear;
    \item p53 signaling pathway: this module includes mutations in TP53, \href{https://en.wikipedia.org/wiki/Cadherin-1}{CDH1}, \href{https://en.wikipedia.org/wiki/GATA3}{GATA3}, \href{https://en.wikipedia.org/wiki/CTCF}{CTCF}, and \href{https://en.wikipedia.org/wiki/GPRIN2}{GPRIN2}, affecting 56\% of samples; this module relates to known breast cancer-related genes involved in metastasis and proliferation, and it does not have any known interactions;
    Module 3: Features mutations in MAP2K4, MAP3K1, PPEF1, SMARCA4, and WWP2, present in 44.4% of samples. This module is linked to the p38-JNK1 stress kinase pathway and includes both kinases and a phosphatase, though interactions within this module are minimal.

    Module 4: Comprises CCND1 amplification and mutations in MAP2K4, RB1, and GRID1, found in 36.3% of samples. It involves cell cycle progression, with CCND1 and RB1 interacting directly. The module also includes mutations in MAP2K4, with limited interaction evidence.
\end{itemize}

Subtype-Specific Findings:

    The study also analyzed data from different breast cancer subtypes (Luminal A/B, Basal, and HER2-enriched). They found that mutual exclusivity of certain gene pairs (e.g., TP53 and GATA3) was consistent across subtypes, suggesting broader significance. However, mutual exclusivity between 12p13.33 amplification and PIK3CA mutations was subtype-specific, indicating that such exclusivity can be influenced by mutational and disease heterogeneity.

\section{C3}

\subsection{Multiple versions}

\textcite{c3} define three methods for assigning weights to the graph to perform the vertex clustering algorithm called C3 (\textit{knowledge-based} \cite{survey}):

\begin{enumerate}
    \item \textbf{ME-CO}, where $w^-$ depends on \textit{mutual exclusivity} and $w^+$ depends on \textit{coverage};
    \item \textbf{NI-ME-CO}, where $w^-$ depends on \textit{mutual exclusivity} and $w^+$ depends on \textit{coverage} and \textit{network information};
    \item \textbf{EX-ME-CO}, where $w^-$ depends on \textit{mutual exclusivity} and $w^+$ depends on \textit{coverage}, \textit{network information} and \textit{expression data}.
\end{enumerate}

As mentioned earlier in \cref{neg_weights}, $w^-$ depends solely on the mutual exclusivity component, while the value of $w^+$ depends on the version of the algorithm chosen.

\subsection{The standard version}

\begin{definition}[ME-CO]
    In the \textbf{ME-CO} version of the algorithm, the following definitions apply:

    \begin{equation}
        \forall u, v \in V(G) \quad w_{uv}^- := w^-_{uv}(\mathrm e)
    \end{equation}

    \begin{equation}
        \forall u, v \in V(G) \quad w_{uv}^+ := w^+_{uv}(\mathrm c)
    \end{equation}
\end{definition}

The definitions for $w_{uv}^-(\mathrm e)$ and $w_{uv}^+(\mathrm c)$ are provided in \cref{me_comp} and \cref{co_comp} respectively. \todo{qui menzionano che se succede una certa cosa fanno un rescaling, lo menziono?}

\subsection{Integrating network information}

Pan-cancer studies, as reported in multiple papers, have demonstrated a significant relationship between network topology and the distribution patterns of cancer drivers. Specifically, the impact of deleterious mutations on the phenotype can be mitigated by certain configurations of the corresponding protein complexes, while other arrangements can amplify their effect. For example, most variants found in healthy individuals tend to be located at the periphery of the interactome, where they do not affect network connectivity. In contrast, cancer-driver somatic mutations are more likely to occur in central, internal regions of the interactome and within highly integrated components.

To precisely quantify the network distances between driver variants, \textcite{c3} computed the pairwise network distances between genes within a large pathway, comprising 8726 genes, by using an implementation of the standard \href{https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm}{Dijkstra algorithm}. To reduce the computational cost of running Dijkstra's algorithm $O\rbk{8726^2}$ times, 1000 pairs were randomly selected for this test. Using the most comprehensive known driver list from the Cancer Gene Census (CGC) \cite{cgc}, the same distances were calculated for driver genes, this time for all gene pairs. The resulting distribution of shortest paths is shown in Figure 1 \todo{non ho inserito l'immagine ma volendo la inserisco, la metto?}, revealing that the average shortest distance between drivers is significantly smaller than that between two randomly selected genes.

placeholder. \todo{per la foto eventualmente}

These findings indicate that network distance and connectivity information should be considered when identifying potential driver mutations. This can be achieved by adjusting the positive weight of edges connecting two genes: if both endpoint genes are drivers, they should be sufficiently central within a given pathway, close to other known drivers, or to each other.

From the KEGG Database \cite{kegg}, a (rather sparse) undirected graph $G'$ was retrieved, where each vertex represents a gene and the edges describe interactions between them. Note that $\abs{V(G)} = \abs{V \left(G' \right)} = n$. For each vertex $u \in V \left(G' \right)$, let $\mathscr{N}(u)$ denote the set of neighbors of $u$, and let $\mathscr{N}'(u) = \mathscr{N}(u) \cup \{u\}$. Also, let

\begin{equation}
    f(u, v) := \dfrac{\abs{\mathscr{N}'(u) \cap \mathscr{N}'(v)}}{\abs{\mathscr{N}'(u) \cup \mathscr{N}'(v)}}
\end{equation}

which is known as the \href{https://en.wikipedia.org/wiki/Jaccard_index}{Jaccard similarity coefficient}; a large value of $f(u, v)$ indicates that $u$ and $v$ are well connected in $G'$ and are likely involved in the same pathway, suggesting that they should be clustered together. Furthermore, let

\begin{equation}
    \mathscr{F} := \{f(u, v) \mid u, v \in V\rbk{G'}\}
\end{equation}

and let $T'(J')$ be the $J'$-the percentile of the values in $\mathscr F$.

\begin{definition}[Network information component]
    The \textbf{network information component} is defined as follows: $$w_{uv}^+(\mathrm n) := \soe{ll}{1 & f(u, v) > T'(J') \\ \dfrac{f(u, v)}{T'(J')} & f(u, v) \le T'(J')}$$
\end{definition}

\begin{definition}[NI-ME-CO]
    The \textbf{NI-ME-CO} version of the algorithm is defined by the following equations:

    \begin{equation}
        \forall u, v \in V(G) \quad w_{uv}^- := w_{uv}^-(\mathrm e)
    \end{equation}

    \begin{equation}
        \forall u, v \in V(G) \quad w_{uv}^+ := w_1 w_{uv}^+(\mathrm c) + w_2 w_{uv}^+(\mathrm n)
    \end{equation}

    where $w_1, w_2 \ge 0$ and $w_1 + w_2 = 1$.
\end{definition}

placeholder. \todo{parlare del rescaling?}

\subsection{Integrating expression data}

Expression data may be integrated through the positive weights, based on the assumption that co-expressed genes are likely to be involved in the same function or cancer pathway. Therefore, genes with strong positive or negative co-expression should be encouraged to cluster together.

Given a vertex $u \in V(G)$, let $\mathrm {\mathbf z(u)}$ be the vector of the time-evolving expression values of $u$ \todo{la prima volta che lessi questo paper non trovai niente sul dove presero queste informazioni, e tutt'ora non mi pare che lo menzionino da nessuna parte; scrivono solo che le informazioni le prendono dal TCGA e dal KEGG, suppongo a questo punto che queste info siano ottenibili dal TCGA ma dovrei controllare manualmente}. Thus, let

\begin{equation}
    g(u, v) := \dfrac{\abs{\abk{\mathrm{\mathbf z(u)}, \mathrm{\mathbf z(v)}}}}{\abs{\abs{\mathrm{\mathbf z(u)}}} \abs{\abs{\mathrm{\mathbf z(v)}}}}
\end{equation}

where $\abk{\mathrm{\mathbf a}, \mathrm{\mathbf b}}$ denotes the inner product of the vectors $\mathrm{\mathbf a}$ and $\mathrm{\mathbf b}$, while $\abs{\abs{\mathrm{\mathbf a}}}$ stands for its $L^2$ norm. This equation is known as the \href{https://en.wikipedia.org/wiki/Cosine_similarity}{cosine similarity}, since the ratio that defines $g(u, v)$ is equal to the cosine of the angle between $\mathrm {\mathbf z(u)}$ and $\mathrm {\mathbf z(v)}$ --- the only difference being the absolute value in the numerator to capture both positive and negative correlations \todo{questa frase l'ho copiata da loro ma non capisco in che senso}. A large value of $g(u, v)$ suggests that the expression vectors of $u$ and $v$ are highly correlated, hence they should be clustered together. Note that $$\forall u, v \in V(G) \quad 0 \le g(u, v) \le 1$$ Moreover, let

\begin{equation}
    \mathscr G := \{g(u, v) \mid u, v \in V(G)\}
\end{equation}

and let $T''(J'')$ be the $J''$-th percentile of the values in $\mathscr G$.

\begin{definition}[Expression data component]
    The \textbf{expression data component} is defined as follows: $$w_{uv}^+(\mathrm x) := \soe{ll}{1 & g(u, v) > T''(J'') \\ \dfrac{g(u, v)}{T''(J'')} & g(u, v) \le T''(J'')}$$
\end{definition}

\begin{definition}[EX-ME-CO]
    The \textbf{EX-ME-CO} version of the algorithm is defined by the following equations:

    \begin{equation}
        \forall u, v \in V(G) \quad w_{uv}^- := w_{uv}^-(\mathrm e)
    \end{equation}

    \begin{equation}
        \forall w_{uv}^+ := w_1 w_{uv}^+(\mathrm c) + w_2 w_{uv}^+(\mathrm x)
    \end{equation}

    where $w_1, w_2 \ge 0$ and $w_1 + w_2 = 1$.
\end{definition}

placeholder. \todo{parlare del rescaling?}

\subsection{Other versions}

\textcite{c3} also mention that other combinations can be used, with appropriate adjustments to the weights, such as the following version, which will be referred to as NI-EX-ME-CO \todo{loro non danno un nome ma glielo sto dando io, è un problema?}.

\begin{definition}[NI-EX-ME-CO]
    The \textbf{NI-EX-ME-CO} version of the algorithm is defined by the following equations:

    \begin{equation}
        \forall u, v \in V(G) \quad w_{uv}^- := w_{uv}^-(\mathrm e)
    \end{equation}

    \begin{equation}
        \forall w_{uv}^+ := w_1 w_{uv}^+(\mathrm c) + w_2 w_{uv}^+(\mathrm n) + w_3 w_{uv}^+(\mathrm x)
    \end{equation}

    where $w_1, w_2, w_3 \ge 0$ and $w_1 + w_2 + w_3 = 1$.
\end{definition}

\subsection{The clustering ILP}

The classical formulation of correlation clustering does not impose any restrictions on cluster sizes. However, all known driver identification methods inherently include cluster size limits, as these sizes directly affect the computational complexity of the algorithms \todo{portano l'esempio di un altro algoritmo chiamato CoMEt che oltre size 10-12 "fails to operate", ha senso inserirlo? forse si}. Therefore, \textcite{c3} introduce a cluster size constraint by assuming that all clusters are of size $k$ at most; clearly, setting $k$ equal to the total number of vertices effectively removes this constraint, allowing flexibility in cluster size selection.

Another reason for imposing a cluster size limit is the expectation that driver genes of specific cancer types will be grouped together, and recent findings indicate that only a small number of drivers are typically present in any given cancer type \todo{non menzionano nessuna fonte dalla quale tirano fuori questa informazione, ritrovatela da solo e aggiungi qualcosa}. If clusters are too large, they may include drivers from multiple cancer types, hiding the detailed separation of the drivers. Furthermore, introducing cluster size constraints helps to avoid the limitations of many clustering algorithms that often produce non-informative \curlyquotes{giant clusters} or singleton clusters \todo{di nuovo, non ci sono citazioni o esempi menzionati, ma vorrei mettecene}.

\begin{definition}[C3's ILP] \label{c3_ilp}
    The \textbf{C3 algorithm} can be defined by the following ILP:

    \begin{equation} \label{c3_first}
        \mathrm{minimize} \sum_{e \in E(G)} (w_e^+x_e + w_e^-(1 - x_e)),
    \end{equation}

    \begin{equation} \label{c3_second}
        \mathrm{subject \ to \ } x_{uv} \le x_{uz} + x_{zv}, \ u, v, z \in V(G) \mathrm{\ distinct},
    \end{equation}

    \begin{equation} \label{c3_third}
        \sum_{v \in V(G) \atop u \neq v} (1 - x_{uv}) \le k, \ u \in V(G),
    \end{equation}

    \begin{equation} \label{c3_fourth}
        x_e \in \{0, 1\}, \ e \in E(G).
    \end{equation}
\end{definition}

In this formulation, $x_e$ allow to describe any clustering of the vertices of $G$, since $x_e \in \{0, 1\}$; also, note that \cref{c3_first} aligns with the definition provided in \cref{c3_chap2}, in fact $x_{uv} = 1$ implies that $u$ and $v$ should belong to different clusters, while $x_{uv} = 0$ implies that the two vertices should be placed into the same cluster. Furthermore, \cref{c3_third} states that for a fixed vertex $u \in V(G)$, the number of variables $x_{uv}$ equal to 0 --- for any $v \in V(G)$ such that $u \neq v$, meaning that $u$ and $v$ are adjacent and in the same cluster --- must not exceed $k$. Lastly, \cref{c3_second} is the \href{https://en.wikipedia.org/wiki/Triangle_inequality}{triangle inequality}, which ensures that if $u$ and $z$ are placed in the same cluster, and $z$ and $v$ are also placed in the same cluster, then $u$ and $v$ will be placed in the same clustered. This means that belonging to the same cluster is a transitive property \todo{dopo aver scritto questo mi sono fatto una domanda alla quale non trovo risposta nel paper: i cluster si possono intersecare? io non direi altrimenti questa definizione sottoforma di ILP non credo potrebbe avere senso, giusto?}, since $$\soe{l}{x_{uz} = 0 \\ x_{zv} = 0 \\ x_{uv} \le x_{uz} + x_{zv}} \implies x_{uv} = 0$$

\subsection{The rounding procedure}

Since solving ILPs is NP-Hard, \textcite{c3} relax the problem by changing \cref{c3_fourth} to an interval constraint $$0 \le x_e \le 1$$ leading to an LP program, the solution of which may be fractional. Hence, to obtain a valid clustering, the fractional solutions have to be rounded. Therefore, instead of solving the LP, \textcite{c3} remove \cref{c3_third} from the linear program, and employ the following rounding procedure.

\begin{algorithm}[H]
    \caption{
        \textit{Rounding procedure}: given a solution $\{x_e\}_{e \in E(G)}$ of \cref{c3_ilp} (without the size constraint \cref{c3_third}), a rational value $\alpha$, and an integer $k$, the algorithm rounds the solution to integer values.
    }

        \label{rounding_procedure}
    \begin{algorithmic}[1]
        \Function{roundingProcedure}{$G$, $\{x_e\}_{e \in E(G)}$, $\alpha$, $k$}
            \State $\mathcal C := \varnothing$ \Comment{the output set of clusters}
            \State $S := V(G)$
            \While{$S \neq \varnothing$}
                \State Choose an arbitrary $u \in S$ \Comment{this is the \textit{pivot vertex}}
                \State $T := \{w \in S - \{u\} \mid x_{uw} \le \alpha\}$ \Comment{$u$'s neighbors under $\alpha$'s threshold}
                \If{$\sum_{w \in T}{x_{uw}} \ge \frac{\alpha}{2}\abs{T}$}
                    \State $\mathcal C = \mathcal C \cup \{u\}$ \Comment{add a singleton cluster $\{u\}$}
                    \State $S = S - \{u\}$
                \ElsIf{$\abs{T} \le k$}
                    \State $\mathcal C = \mathcal C \cup \rbk{\{u\} \cup T}$ \Comment{add the cluster $(\{u\} \cup T)$}
                    \State $S = S - \rbk{\{u\} \cup T}$
                \Else
            \State Partition $T$ into $\{T_0', T_1, \ldots, T_p\}$, such that: \begin{itemize} \item[] \quad \quad \quad \quad $\sbullet \ \abs{T_0'} = k$ \item[] \quad \quad \quad \quad $\sbullet \ \abs{T_i} = k + 1$ for each $0 < i < p$ \item[] \quad \quad \quad \quad $\sbullet \ \abs{T_p} \le k + 1$ \end{itemize}
                    \State $T_0 := T_0' \cup \{u\}$ \Comment{$T_0$ has $k + 1$ elements}
                    \For{$i \in [0, p]$}
                        \State $\mathcal C = \mathcal C \cup T_i$ \Comment{add each partition as a cluster}
                    \EndFor
                    \State $S = S - \rbk{\{u\} \cup T}$
                \EndIf
            \EndWhile
            \State \textbf{return} $\mathcal C$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

placeholder. \todo{volevo dare una spiegazione a parole dell'algoritmo ma dopo averlo scritto mi sono reso conto che è abbastanza autoesplicativo, l'unica cosa che è un po vaga è la condizione della riga 7 che non comprendo del tutto}

placeholder. \todo{nel materiale supplementare trattano di un valore ottimale per $\alpha$, lo vedo?}

% The algorithm starts by defining an empty set $\mathcal C$, which will contain the clusters of $G$, and a set $S = V(G)$, which will be used to partition $V(G)$. Until $S$ is not empty, repeat the following:
%
% \begin{itemize}
%     \item choose an arbitrary vertex $u \in S$, which will be referred to as the \textit{pivot vertex};
%     \item define $T$ to be the set of the pivot's neighbors $w$ such that $x_{uw}$ is less than the given threshold $\alpha$;
%     \item if the sum of the elements of $T$ is more than half of the elements comprising $T$, scaled by $\alpha$ \todo{non capisco perché moltiplicano per $\alpha$}, then add a singleton cluster $\{u\}$ to $\mathcal C$ and remove the pivot from $S$;
%     \item otherwise, if the number of filtered neighbors of the pivot is at most $k$, create a $\{u\} \cup T$ cluster and remove every one of its vertices from $S$;
%     \item if none of the previous conditions were met, then $T$ must be partitioned into $k + 1$ sized clusters, one of which must contain the pivot (and there may be a set with fewer vertices).
% \end{itemize}

\begin{definition}[C3] \label{c3_final}
    The \textbf{C3 algorithm} is defined as follows: first, the next ILP is solved

    \begin{equation}
        \mathrm{minimize} \sum_{e \in E(G)} (w_e^+x_e + w_e^-(1 - x_e)),
    \end{equation}

    \begin{equation}
        \mathrm{subject \ to \ } x_{uv} \le x_{uz} + x_{zv}, \ u, v, z \in V(G) \mathrm{\ distinct},
    \end{equation}

    \begin{equation}
        0 \le x_e \le 1, \ e \in E(G).
    \end{equation}

    and then the rounding procedure defined in \cref{rounding_procedure} is applied.

\end{definition}

\cleardoublepage
