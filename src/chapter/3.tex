\chapter{Finding driver mutations} \label{chap:finding_driver_mutations}

Although the true explanation for mutual exclusivity remains unknown, and its therapeutic potential is still uncertain, this phenomenon is frequently observed in data and may lead to discoveries in cancer treatment. Existing approaches can be categorized into two types: \textbf{\textit{de novo}} approaches, which identify mutually exclusive patterns using only genomic data from patients, and \textbf{\textit{knowledge-based}} methods, which integrate the analysis with external \textit{a priori} information \cite{survey}. \textit{De novo} approaches might lack sufficient information as they do not utilize existing databases \todo{correggi questa frase che non ha senso effettivamente}. Conversely, given that our understanding of gene and protein interactions in humans is still incomplete and many pathway databases fail to accurately represent the specific pathways and interactions present in cancer cells, \textit{knowledge-based} approaches may be limited by their dependence on existing data sources. Consequently, \textit{de novo} methods might yield new but potentially less accurate results, while \textit{knowledge-based} approaches may limit the discovery of novel biological insights \cite{multi-dendrix}.

\section{Dendrix}

\subsection{A greedy approach}

To find a solution to the problem described in \cref{mwsp}, \textcite{dendrix} developed the following greedy algorithm called Dendrix (\textit{de novo} \cite{survey}).

\begin{algorithm}[H]
    \caption{
        \textit{Greedy Dendrix}: given the set of all genes $\mathcal G$, and an integer $k$, the algorithm finds the set of genes $M$ of size $k$ that maximizes $W(M)$.
    }

        \label{greedy_dendrix}
    \begin{algorithmic}[1]
        \Function{greedyDendrix}{$\mathcal G$, $k$}
            \State $M := \{g_1, g_2\}$ such that $M$ maximizes $W(M)$ \Comment{pick the best gene pair}
            \For{$i \in [3, k]$}
                \State $\hat g := \argmax_{g \in \mathcal G}{W \rbk{M \cup \{g\}}}$ \Comment{TODO A PARIMERITO???}
                \State $M = M \cup \{\hat g\}$
            \EndFor
            \State \textbf{return} $M$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

The time complexity of the algorithm is $O\rbk{n^2 + kn} = O\rbk{n^2}$. \todo{explain briefly why}

While this algorithm is efficient, there is generally no guarantee that it will identify the optimal set $\hat M$ that maximizes $W(\hat M)$. However, \textcite{dendrix} prove that \cref{greedy_dendrix} can correctly identify $\hat M$ with high probability when the mutation data come from the \textit{Gene Independence Model} (GIM), which is described below. \todo{lo dimostrano nel materiale supplementare, lo vedo/metto? in caso mettilo dopo la definizione}

\begin{definition}[Gene Independence Model]
    Let $A$ be an $m \times n$ mutation matrix such that $\hat M$ is the \textit{maximum weight submatrix} of $A$ and $\abs{\hat M} = k$; the matrix $A$ satisfies the \textbf{Gene Independence Model} (GIM) if and only if:

    \begin{enumerate}[label=\roman*), font=\itshape]
        \item each gene $g \notin \hat M$ is mutated in each patient with probability $p_g \in \sbk{p_L, p_U}$ \todo{WHAT ARE THESE??}, independently of all other events;
        \item $W (\hat M)= \Omega(m)$; \todo{qua scrivono che la definizione di $Omega$ è che $W (\hat M) = rm$ per $0 < r \le 1$???????}
        \item for all $M \subset \hat M$ of cardinality $l := \abs{M}$, it exists $0 \le d < 1$ such that $W(M) \le \frac{l + d}{k} W(\hat M)$.
    \end{enumerate}
\end{definition}

Note that:

\begin{itemize}
    \item condition $(i)$ reflects the independence of mutations for genes outside the mutated pathway, a standard assumption for somatic single-nucleotide mutations;
    \item condition $(ii)$ ensures that mutations in $\hat M$ cover a large number of patients and are mostly exclusive;
    \item condition $(iii)$ means that each gene in $\hat M$ is important, so there are no subset of $\hat M$ that predominantly contributes to $W(\hat M)$.
\end{itemize}

Although it is possible to obtain accurate results with high probability under the GIM, genes within $\hat M$ may exhibit observed mutation frequencies similar to those of genes outside $\hat M$. This similarity can make it challenging to distinguish between them based solely on mutation frequency, regardless of the number of patients. In practical applications, the utility of this greedy algorithm depends on the availability of mutation data from a sufficient number of patients. Moreover, the GIM model may not be suitable for all types of somatic mutations.

\subsection{Using MCMC}

To address the limitations of the aforementioned greedy algorithm, \textcite{dendrix} developed a \href{https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo}{\textit{Markov Chain Monte Carlo}} (MCMC) approach, which does not rely on assumptions about the distribution of mutation data or the number of patients. 

TODO AGGIUNGI PARAGRAFO

Additionally, the MCMC method does not assume independence among mutations in different genes, making it particularly useful for analyzing copy-number aberrations (CNAs), which often involve correlated mutations due to amplification or deletion of adjacent genes.

\begin{definition}[Dendrix's MCMC]
    The \textbf{MCMC's procedure} of Dendrix is defined through the following steps:

    \begin{enumerate}
        \item \textit{initialization}: given the set of all genes $\mathcal G$, choose an arbitrary subset $M_0 \subseteq \mathcal G$ of size $k$ genes;
        \item \textit{iteration}: for $t = 1, 2, \ldots$, derive $M_{t + 1}$ from $M_t$ as follows:
            \begin{enumerate}
                \item choose a gene $w$ uniformly, at random, from $\mathcal G$;
                \item choose a gene $v$ uniformly, at random, from $M_t$;
                \item let $M_t' := (M_t - \{v\}) \cup \{w\}$;
                \item let $\Delta_W := W\rbk{M_t'} - W(M_t)$;
                \item let $P(M_t, w, v) := \min \sbk{1, e^{c \Delta_W}}$, where $c > 0$;
                \item set $M_{t+1} := M_t'$ with probability $P(M_t, w, v)$, else $M_{t + 1} := M_t$
            \end{enumerate}
    \end{enumerate}
\end{definition}

Note that:

\begin{itemize}
    \item $w$ is choosen from $\mathcal G$, therefore if $w \in M_t$, then $$M_t' = (M_t - \{v\}) \cup \{w\} = M_t - \{v\}$$ which means that no genes were added to $W_t'$; this must be allowed because maximizing the weight may require removing genes already present in the current set, without adding new ones;
    \item $\Delta_W$ measures the change in the weight function with the new set, and the constant $c$ is a scaling factor that adjusts the importance of this difference; note that $$\Delta_W \ge 0 \implies e^{c \Delta_W} \ge 1 \implies P(M_t, w, v) = 1 \implies M_{t + 1} = M_t'$$ in fact when $\Delta_W > 0$ the weight has improved, therefore the next iteration should start from $M_t'$; conversely $$\Delta_W < 0 \implies e^{c \Delta_W} < 1 \implies P(M_t, w, v) = e ^{c \Delta_W}$$ which means that when $\Delta_W < 0$ (i.e., the weight has decreased) the change will be perfomed with probability $e^{c \Delta_W}$ --- this value is still close to 1 if the weight has not decreased significantly.
\end{itemize}

\section{Multi-Dendrix}

\subsection{An alternative solution to Dendrix}

\textcite{multi-dendrix}, the authors of Multi-Dendrix (\textit{de novo} \cite{survey}), try to solve the same problem posed by \textcite{dendrix}, described in \cref{mwsp}, by formulating the problem as an \textit{Integer Linear Program} (ILP), which they refer to as $\mathrm{Dendrix}_{\mathrm{\textit{ILP}}}(k)$. Consider a gene set $M$ defined by a set of indicator variables, one for each gene $j \in M$, as follows

\begin{equation}
    I_M(j) = 1 \iff j \in M
\end{equation}

and a set of indicator variables, one for each patient $i$, expressed in this form

\begin{equation} \label{c_idefn}
    C_i(M) = 1 \iff \exists g \in M \mid i \in \Gamma(g)
\end{equation}

\begin{definition}[$\mathrm{Dendrix}_{\mathrm{\textit{ILP}}}(k)$]
    $\mathrm{Dendrix}_{\mathrm{\textit{ILP}}}(k)$ is defined by the following ILP:

    \begin{equation} \label{weight_dendrix_ilp}
        \mathrm{maximize} \sum_{i = 1}^m {\rbk{2 \cdot C_i(M) - \sum_{j = 1}^n I_M(j) \cdot a_{i, j}}},
    \end{equation}

    \begin{equation} \label{second_constr_dendrix_ilp}
        \mathrm{subject \ to} \sum_{j = 1}^n{I_M(j) = k},
    \end{equation}

    \begin{equation} \label{third_constr_dendrix_ilp}
        \sum_{j = 1}^n I_M(j) \cdot {a_{i, j}} \ge C_i(M),
    \end{equation}

    \begin{equation*}
        \mathrm{for\ } 1 \le i \le m.
    \end{equation*}
\end{definition}

Note that the sum in \cref{weight_dendrix_ilp} is the second version of the definition provided in \cref{weight}.

\begin{lemma}[Correctness of $\mathrm{Dendrix}_{\mathrm{\textit{ILP}}}(k)$] Given a gene set $M$, the sum in \cref{weight_dendrix_ilp} correctly evaluates $W(M)$.
\end{lemma}

\begin{proof}
    Rearranging the terms in \cref{weight_dendrix_ilp} $$\sum_{i = 1}^m {\rbk{2 \cdot C_i(M) - \sum_{j = 1}^n I_M(j) \cdot a_{i, j}}} = 2\sum_{i = 1}^m {C_i(M)} - \sum_{i = 1}^m {\sum_{j = 1}^n {I_M(j) \cdot a_{i, j}}}$$ and it is trivial to check that $$\abs{\Gamma(M)} = \sum_{i = 1}^m {C_i(M)}$$ since it it true by definition, and $$\sum_{g \in M}{\abs{\Gamma(g)}} = \sum_{i = 1}^m {\sum_{j = 1}^n {I_M(j) \cdot a_{i, j}}}$$ because the RHS counts the number of cells of $A$ such that $a_{i, j} = 1$ for every $j \in M$.
\end{proof}

\cref{second_constr_dendrix_ilp} limits the size of $M$ to be exactly $k$; moreover, note that \cref{third_constr_dendrix_ilp} only forces $C_i(M) = 0$ when the $i$-th patient has no mutated genes in $M$ but does not force $C_i(M) = 1$ when the patient has at least one, as required by \cref{c_idefn}. However, the objective function will be maximized when $C_i(M)=1$ thus \cref{c_idefn} is satisfied.

placeholder. \todo{non mi ricordo cosa volevo scrivere qua}

\subsection{The ILP}

As outlined in \cref{multi_dendrix_2nd_chap}, \textcite{multi-dendrix} propose that the most effective approach to conducting the research is to identify a collection of gene sets that maximize the sum of their individual weights. To achieve this result, they solve the following problem, which is an extension of \cref{mwsp}:

\begin{displayquote}
    \textbf{Multiple Maximum Weight Submatrices Problem}: Given an $m \times n$ mutation matrix $A$, and integer $t > 0$, and two integers $k_{\mathrm{min}}, k_{\mathrm{max}} \ge 0$, find a collection $M = \{M_1, \ldots, M_t\}$ of column submatrices that maximizes

    \begin{equation}
        W'(M) := \sum_{\rho = 1}^t {W(M_\rho)}
    \end{equation}

    where each submatrix $M_\rho$ --- for $1 \le \rho \le t$ ---  has size $m \times k_\rho$ for some $k_{\mathrm{min}} \le k_\rho \le k_{\mathrm{max}}$.
\end{displayquote}

Note that this problem is NP-Hard, as for the case $t = 1$. Furthermore, collections $M$ with a large value of $W'(M)$ are also likely to have higher coverage $\Gamma(M_\rho)$ for each individual gene set $\rho$. As a result, optimal solutions tend to produce collections where many patients have mutations in more than one gene set, or they may be pairs or larger groups of co-occurring mutations, a phenomenon observed in cancer. \todo{qua inseriscono una citazione, potrebbe valere la pena di inserirla e/o indagare?}

\begin{definition}[Multi-Dendrix]
    $\mathrm{Multi}$-$\mathrm{Dendrix}$ is defined by the following ILP:

    \begin{equation} \label{weight_multi-dendrix_ilp}
        \mathrm{maximize} \sum_{\rho = 1}^t {\sum_{i = 1}^m {\rbk{2 \cdot C_i(M_\rho) - \sum_{j = 1}^n I_{M_\rho}(j) \cdot a_{i, j}}}},
    \end{equation}

    \begin{equation} \label{second_constr_multi-dendrix_ilp}
        \mathrm{subject \ to} \sum_{j = 1}^n I_{M_\rho}(j) \cdot {a_{i, j}} \ge C_i(M_\rho),
    \end{equation}

    \begin{equation} \label{multi-dendrix_ilp_size}
        k_\mathrm{min} \le \sum_{j = 1}^n {I_{M_\rho}(j)} \le k_\mathrm{max},
    \end{equation}
    
    \begin{equation*}
        \mathrm{for\ } 1 \le i \le m, \ 1 \le \rho \le t,
    \end{equation*}

    \begin{equation} \label{multi-dendrix_ilp_last}
        \sum_{\rho = 1}^t{I_{M_\rho}(j)} \le 1, \ 1 \le j \le n.
    \end{equation}
\end{definition}

Note that:

\begin{itemize}
    \item \cref{weight_multi-dendrix_ilp} and \cref{second_constr_multi-dendrix_ilp} expand \cref{weight_dendrix_ilp} and \cref{second_constr_dendrix_ilp} respectively;
    \item \cref{multi-dendrix_ilp_size} allows each gene group to have a size between $k_\mathrm{min}$ and $k_\mathrm{max}$;
    \item \cref{multi-dendrix_ilp_last} states that each gene can appear in at most one set within the collection. 
\end{itemize}

\textcite{multi-dendrix} compare the outputs of their ILP with an iterative version of the Dendrix greedy algorithm (discussed in \cref{greedy_dendrix}), which they refer to as Iter-Dendrix, running Dendrix multiple times to produce a collection of gene sets.

\begin{algorithm}[H]
    \caption{
        \textit{Iter-Dendrix}: given the set of all genes $\mathcal G$, an integer $k$, and an integer $t$, the algorithm finds the collection $M$ of $t$ gene sets of size $k$ that maximizes $W'(M)$.
    }

        \label{iter-dendrix}
    \begin{algorithmic}[1]
        \Function{iterDendrix}{$\mathcal G$, $k$, $t$}
            \State $M := \varnothing$
            \For{$i \in [1, t]$}
            \State $M_i := \texttt{greedyDendrix}(\mathcal G, k)$ \Comment{procedure defined in \cref{greedy_dendrix}}
                \State $M = M \cup \{M_i\}$
                \State $\mathcal G = \mathcal G - M_i$
            \EndFor
            \State \textbf{return} $M$
        \EndFunction
    \end{algorithmic}
\end{algorithm}


Denoting with $M$ and $I$ the collections of gene sets obtained from Multi-Dendrix and Iter-Dendrix respectively, \textcite{multi-dendrix} state that $W'(M) \ge W'(I)$ \todo{loro dicono che sia ovvio ma non capisco perché dovrebbe essere così ovvio}. They also argue that $M$ could contain sets with strictly greater weight than comprising $I$ due to several factors:

\begin{itemize}
    \item there may be multiple gene sets $I_\rho$ that maximize $W(I_\rho)$ on the $\rho$-th iteration of Iter-Dendrix, and this version of Dendrix can only extend one of these sets;
    \item the gene set $I_\rho$ that maximizes $W(I_\rho)$ selected by Iter-Dendrix in the $\rho$-th iteration may not be a member of $M$, since $M$ could include gene sets that are suboptimal when considered in isolation;
    \item when $k_\mathrm{min} < k_\mathrm{max}$ Multi-Dendrix may choose gene sets with fewer than $k_\mathrm{max}$ genes if doing so maximizes the overall weight $W'(M)$.
\end{itemize}

\textcite{multi-dendrix} state that all of these scenarios occur when analyzing real mutation data.

Lastly, Multi-Dendrix can be extended to allow gene sets to overlap, since the genes in the intersection may be involved in multiple biological processes. Hence, \cref{multi-dendrix_ilp_last} is replaced with the following equation:

\begin{equation}
    \sum_{\rho = 1}^t {I_{M_\rho}(j) \le \Delta}, \quad 1 \le j \le n
\end{equation}

where $\Delta$ is the maximum number of gene sets a gene can be a member of, and the following constraint is added:

\begin{equation}
    \sum_{j = 1}^n {\sum_{\rho = 1}^t \sum_{\rho' = 1 \atop \rho \neq \rho'}^t {{I_{M_\rho} (j) \cdot I_{M_{\rho'}} (j)}}} \le \tau, \quad 1 \le \rho \le t
\end{equation}

where $\tau$ is the maximum size of the intersection between two gene sets.

\section{MDPFinder}

\subsection{The genetic algorithm}

placeholder. \todo{sezione in cui scrivo che mpdfinder critica la soluzione esatta trovata da multi-dendrix, che nonostante sia esatta potrebbe non essere reale; in questa sezione va menzionato che l'algoritmo si chiama MDPFinder, e va scritto che è knowledge-based}

The approach used by \textcite{mdpfinder} involves a \textit{Genetic Algorithm} (GA), which is versatile and flexible, and can be used to optimize a wide variety of scoring functions. In addition, they state that the GA approach is particularly relevant to the current problem due to its conceptual alignment with the notions of \textit{gene} and \textit{mutation}. It models genetic variation within a population, evolving through a process of random selection, thereby avoiding the need to enumerate all possible solutions.

\begin{definition}[Hypothesis space]
    A \textbf{member} of the population is defined by a binary string of length $n$, i.e. the number of genes. Given a gene set $M$, the value of the $i$-th position of an individual represents the membership of the $i$-th gene in $M$.

    Therefore, the \textbf{hypothesis space} is constituted by all the possible binary strings with length $n$ that have $k$ 1s, namely $$\mathcal H = \left\{(x_1, \ldots, x_n) \mid x_i \in \{0, 1\}, i \in [1, n], \sum_{j = 1}^n {x_j} = k \right\}$$
\end{definition}

\begin{definition}[Fitness function]
    The \textbf{fitness} $f_i$ of each individual $h_i$ (its corresponding gene set is $M_i$) of the population is defined as the rank $r_i$ of the score $W(M_i)$, in the ascending order \todo{SECONDO ME È DESCENDING ALTRIMENTI PRENDI QUELLO COL PESO MINORE CHE NON HA SENSO, probabilmente non sto capendo il senso della frase}: $$\forall h_i \in \mathcal H \quad f_i := r_i$$
\end{definition}

\begin{definition}[Selection probability] \label{selection_probability}
    Given the rank $r_i$ of an individual $h_i$ based on its score, the \textbf{selection probability} is defined as follows: $$p_i = \dfrac{2r_i}{P(P + 1)}$$ where $P$ is the population size. The individual with the highest fitness value is most likely to be transferred into the next generation.
\end{definition}

This selection operator is based on \href{https://en.wikipedia.org/wiki/Selection_(genetic_algorithm)#Roulette_wheel_selection}{roulette wheel selection}, which states that the probability of choosing an individual is equal to $$p_i = \dfrac{f_i}{\sum_{j = 1}^P {f_j}} = \dfrac{r_i}{\frac{P(P + 1)}{2}} = \dfrac{2r_i}{P(P+1)}$$ which is precisely the equation in \cref{selection_probability}.

\begin{definition}[Crossover operator]
    The \textbf{crossover operator} specifies the breeding process as follows: the offspring inherits the variables shared by both parents, while the non-shared ones are selected from the symmetric difference of the parents' genetic makeup.
\end{definition}

\begin{definition}[Mutation operator]
    The \textbf{mutation operator} randomly sets the value of one variable from 1 to 0, and changes another variable value from 0 to 1, ensuring the feasibility of every offspring.
\end{definition}

\begin{definition}[Local search] \todo{fixa questa definizione}
    To prevent premature convergence and enhance the accuracy of the algorithm, a local search strategy is employed to improve search performance. In particular, the values of two variables are randomly altered, as the mutation operator. If this adjustment improves the current solution, it is accepted; the search is terminated once all variables have been tested with this routine.
\end{definition}

\begin{definition}[GA procedure]
    The following are the details of the \textbf{GA procedure}:

    \begin{enumerate}
        \item \textit{population generation}: a random population of size $P$ and mutation rate $p_m$ is generated, where $P = n$ (i.e. the number of available genes);
        \item \textit{breeding}: for each iteration, $P$ couples are selected from the current population, based on $p_i$, and each couple generates an offspring;
        \item \textit{mutation}: each offspring may optionally receive a mutation with probability $p_m$;
        \item \textit{selection}: all parents and offspring are ranked based on their scoring values, and the top $P$ individuals are selected to form the next generation (this is referred to as \href{https://en.wikipedia.org/wiki/Truncation_selection}{truncation selection});
        \item \textit{local search}: verify if the iteration is stuck in a local solution (e.g. if the maximum scoring value does not improve over two consecutive iterations); if this is the case, perform a local search;
        \item \textit{termination}: proceed as such until the termination criterion is met (e.g. if the current maximum scoring value does not improve over 10 consecutive iterations); if this occurs, then end the procedure.
    \end{enumerate}
\end{definition}

\subsection{The integration procedure}

In practical applications, multiple optimal solutions may exist. Additionally, due to data noise and other factors, the solutions considered optimal --- i.e. the ones with the highest $W(M)$ --- may not necessarily be the most relevant in a biological context. To identify the most biologically meaningful solutions, other types of data are integrated to refine the results. Specifically, the GA procedure is extended by incorporating gene expression data to enhance its performance. The integrative model is developed based on the observation that genes within the same pathway typically collaborate to perform a specific function. Consequently, the expression profiles of gene pairs within the same pathway often exhibit higher correlations than those in different pathways. This characteristic can be leveraged to distinguish between gene sets that have the same score: the model focuses on detecting gene sets whose scores $W(M)$ are close to the optimal solution, but whose member genes display stronger correlations with each other.

\begin{definition}[Integrative measure]
    Given an $m \times n$ mutation matrix $A$, an expression matrix $E$ with the same dimensions \todo{nel paper non è menzionato cosa questa "expression matrix" contenga all'interno, c'è una foto con dei colori e basta, però ho visto che "expression matrix" è un termine che si usa per indicare un tipo di matrici solamente che contengono una cosa ben specifica, posso assumere che si sappia già di cosa si parla o devo spiegare cosa sono?}, and an $A$'s submatrix $M$ of size $m \times k$, the integrative model is defined by the following \textbf{measure}: $$F_{ME} := W(M) + \lambda \cdot R(E_M)$$ where $E_M$ is $E$'s expression submatrix that corresponds to $M$, and $R(E_M)$ is described by the following equation: $$R(E_M) = \sum_{j_1 = 1}^n {\sum_{j_2 = 1 \atop j_1 \neq j_2} ^n {\dfrac{\abs{\mathrm{pcc}(x_{j_1}, x_{j_2})}}{\frac{k(k - 1)}{2}}}}$$ where $\mathrm{pcc}(\cdot)$ is the \href{https://en.wikipedia.org/wiki/Pearson_correlation_coefficient}{Pearson correlation coefficient}, and $x_j$ is the expression profile of gene $j$ \todo{non è spiegato cosa questo voglia dire ma io assumo sia un vettore colonna di $E$; inoltre, non ho idea di cosa sia il valore $R(E_M)$}.
\end{definition}

Note that $$0 \le R(E_m) \le 1$$ and $W(M)$ is an integer, therefore when $\lambda = 1$ the value of $F_{ME}$ can be used to discriminate the gene sets with the same $W(M)$. Moreover, for values of $\lambda \ge 1$, the gene set with strong correlation and approximate exclusivity can be identified.

\section{Mutex}

\subsection{A different greedy method}

To identify the most mutually exclusive group, \textcite{mutex} employ a greedy algorithm called Mutex (\textit{knowledge-based} \cite{survey}), which is applied to a directed graph constructed from databases containing information about biological pathways \todo{menziono i database che hanno usato?}.

The search begins by initializing a set with an altered gene as the seed and then expanding the group greedily with the next best candidate gene. Candidate genes are selected such that, after their addition, the group still has a common downstream gene that can be accessed without passing through any non-member genes (the common downstream gene may also be a member of the group) \todo{ci sono delle figure con un grafo e l'insieme che viene proressivamente espanso dall'algoritmo, forse potrei riprodurle e inserirle per chiarezza?}. The group is expanded with the candidate that improves the group score the most. The process continues until no candidates remain or the group reaches a preset size threshold. The algorithm outputs a group and its score for each seed gene \todo{qui menzionano una cosa inerente al controllo dell'FDR ma per ora la ometto perché non so che significa, credo andrebbe inserita}.

placeholder. \todo{una cosa carina sarebbe scrivere lo pseudocodice dell'algoritmo greedy (sarebbe anche più carino a quel punto dimostrarne la correttezza ma non credo sia possibile vista la natura statistica della ricerca effettuata), loro non lo forniscono e lo trattano solo a parole, provo a farlo?}

\begin{definition}[Proximity]
    Given a gene graph, the \textbf{proximity} of a gene $G$ includes not only the genes directly adjacent to $g$ but also those that share downstream targets in the pathway with $g$. \todo{QUESTA DEFINIZIONE NON VA QUA}
\end{definition}

\section{C3}

\subsection{Multiple versions}

\textcite{c3} define three methods for assigning weights to the graph to perform the vertex clustering algorithm called C3 (\textit{knowledge-based} \cite{survey}):

\begin{enumerate}
    \item \textbf{ME-CO}, where $w^-$ depends on \textit{mutual exclusivity} and $w^+$ depends on \textit{coverage};
    \item \textbf{NI-ME-CO}, where $w^-$ depends on \textit{mutual exclusivity} and $w^+$ depends on \textit{coverage} and \textit{network information};
    \item \textbf{EX-ME-CO}, where $w^-$ depends on \textit{mutual exclusivity} and $w^+$ depends on \textit{coverage}, \textit{network information} and \textit{expression data}.
\end{enumerate}

As mentioned earlier in \cref{neg_weights}, $w^-$ depends solely on the mutual exclusivity component, while the value of $w^+$ depends on the version of the algorithm chosen.

\subsection{The standard version}

\begin{definition}[ME-CO]
    In the \textbf{ME-CO} version of the algorithm, the following definitions apply:

    \begin{equation}
        \forall u, v \in V(G) \quad w_{uv}^- := w^-_{uv}(\mathrm e)
    \end{equation}

    \begin{equation}
        \forall u, v \in V(G) \quad w_{uv}^+ := w^+_{uv}(\mathrm c)
    \end{equation}
\end{definition}

The definitions for $w_{uv}^-(\mathrm e)$ and $w_{uv}^+(\mathrm c)$ are provided in \cref{me_comp} and \cref{co_comp} respectively. \todo{qui menzionano che se succede una certa cosa fanno un rescaling, lo menziono?}

\subsection{Integrating network information}

Pan-cancer studies, as reported in multiple papers, have demonstrated a significant relationship between network topology and the distribution patterns of cancer drivers. Specifically, the impact of deleterious mutations on the phenotype can be mitigated by certain configurations of the corresponding protein complexes, while other arrangements can amplify their effect. For example, most variants found in healthy individuals tend to be located at the periphery of the interactome, where they do not affect network connectivity. In contrast, cancer-driver somatic mutations are more likely to occur in central, internal regions of the interactome and within highly integrated components.

To precisely quantify the network distances between driver variants, \textcite{c3} computed the pairwise network distances between genes within a large pathway, comprising 8726 genes, by using an implementation of the standard \href{https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm}{Dijkstra algorithm}. To reduce the computational cost of running Dijkstra's algorithm $O\rbk{8726^2}$ times, 1000 pairs were randomly selected for this test. Using the most comprehensive known driver list from the Cancer Gene Census (CGC) \cite{cgc}, the same distances were calculated for driver genes, this time for all gene pairs. The resulting distribution of shortest paths is shown in Figure 1 \todo{non ho inserito l'immagine ma volendo la inserisco, la metto?}, revealing that the average shortest distance between drivers is significantly smaller than that between two randomly selected genes.

placeholder. \todo{per la foto eventualmente}

These findings indicate that network distance and connectivity information should be considered when identifying potential driver mutations. This can be achieved by adjusting the positive weight of edges connecting two genes: if both endpoint genes are drivers, they should be sufficiently central within a given pathway, close to other known drivers, or to each other.

From the KEGG Database \cite{kegg}, a (rather sparse) undirected graph $G'$ was retrieved, where each vertex represents a gene and the edges describe interactions between them. Note that $\abs{V(G)} = \abs{V \left(G' \right)} = n$. For each vertex $u \in V \left(G' \right)$, let $\mathscr{N}(u)$ denote the set of neighbors of $u$, and let $\mathscr{N}'(u) = \mathscr{N}(u) \cup \{u\}$. Also, let

\begin{equation}
    f(u, v) := \dfrac{\abs{\mathscr{N}'(u) \cap \mathscr{N}'(v)}}{\abs{\mathscr{N}'(u) \cup \mathscr{N}'(v)}}
\end{equation}

which is known as the \href{https://en.wikipedia.org/wiki/Jaccard_index}{Jaccard similarity coefficient}; a large value of $f(u, v)$ indicates that $u$ and $v$ are well connected in $G'$ and are likely involved in the same pathway, suggesting that they should be clustered together. Furthermore, let

\begin{equation}
    \mathscr{F} := \{f(u, v) \mid u, v \in V\rbk{G'}\}
\end{equation}

and let $T'(J')$ be the $J'$-the percentile of the values in $\mathscr F$.

\begin{definition}[Network information component]
    The \textbf{network information component} is defined as follows: $$w_{uv}^+(\mathrm n) := \soe{ll}{1 & f(u, v) > T'(J') \\ \dfrac{f(u, v)}{T'(J')} & f(u, v) \le T'(J')}$$
\end{definition}

\begin{definition}[NI-ME-CO]
    The \textbf{NI-ME-CO} version of the algorithm is defined by the following equations:

    \begin{equation}
        \forall u, v \in V(G) \quad w_{uv}^- := w_{uv}^-(\mathrm e)
    \end{equation}

    \begin{equation}
        \forall u, v \in V(G) \quad w_{uv}^+ := w_1 w_{uv}^+(\mathrm c) + w_2 w_{uv}^+(\mathrm n)
    \end{equation}

    where $w_1, w_2 \ge 0$ and $w_1 + w_2 = 1$.
\end{definition}

placeholder. \todo{parlare del rescaling?}

\subsection{Integrating expression data}

Expression data may be integrated through the positive weights, based on the assumption that co-expressed genes are likely to be involved in the same function or cancer pathway. Therefore, genes with strong positive or negative co-expression should be encouraged to cluster together.

Given a vertex $u \in V(G)$, let $\mathrm {\mathbf z(u)}$ be the vector of the time-evolving expression values of $u$ \todo{la prima volta che lessi questo paper non trovai niente sul dove presero queste informazioni, e tutt'ora non mi pare che lo menzionino da nessuna parte; scrivono solo che le informazioni le prendono dal TCGA e dal KEGG, suppongo a questo punto che queste info siano ottenibili dal TCGA ma dovrei controllare manualmente}. Thus, let

\begin{equation}
    g(u, v) := \dfrac{\abs{\abk{\mathrm{\mathbf z(u)}, \mathrm{\mathbf z(v)}}}}{\abs{\abs{\mathrm{\mathbf z(u)}}} \abs{\abs{\mathrm{\mathbf z(v)}}}}
\end{equation}

where $\abk{\mathrm{\mathbf a}, \mathrm{\mathbf b}}$ denotes the inner product of the vectors $\mathrm{\mathbf a}$ and $\mathrm{\mathbf b}$, while $\abs{\abs{\mathrm{\mathbf a}}}$ stands for its $L^2$ norm. This equation is known as the \href{https://en.wikipedia.org/wiki/Cosine_similarity}{cosine similarity}, since the ratio that defines $g(u, v)$ is equal to the cosine of the angle between $\mathrm {\mathbf z(u)}$ and $\mathrm {\mathbf z(v)}$ --- the only difference being the absolute value in the numerator to capture both positive and negative correlations \todo{questa frase l'ho copiata da loro ma non capisco in che senso}. A large value of $g(u, v)$ suggests that the expression vectors of $u$ and $v$ are highly correlated, hence they should be clustered together. Note that $$\forall u, v \in V(G) \quad 0 \le g(u, v) \le 1$$ Moreover, let

\begin{equation}
    \mathscr G := \{g(u, v) \mid u, v \in V(G)\}
\end{equation}

and let $T''(J'')$ be the $J''$-th percentile of the values in $\mathscr G$.

\begin{definition}[Expression data component]
    The \textbf{expression data component} is defined as follows: $$w_{uv}^+(\mathrm x) := \soe{ll}{1 & g(u, v) > T''(J'') \\ \dfrac{g(u, v)}{T''(J'')} & g(u, v) \le T''(J'')}$$
\end{definition}

\begin{definition}[EX-ME-CO]
    The \textbf{EX-ME-CO} version of the algorithm is defined by the following equations:

    \begin{equation}
        \forall u, v \in V(G) \quad w_{uv}^- := w_{uv}^-(\mathrm e)
    \end{equation}

    \begin{equation}
        \forall w_{uv}^+ := w_1 w_{uv}^+(\mathrm c) + w_2 w_{uv}^+(\mathrm x)
    \end{equation}

    where $w_1, w_2 \ge 0$ and $w_1 + w_2 = 1$.
\end{definition}

placeholder. \todo{parlare del rescaling?}

\subsection{Other versions}

\textcite{c3} also mention that other combinations can be used, with appropriate adjustments to the weights, such as the following version, which will be referred to as NI-EX-ME-CO \todo{loro non danno un nome ma glielo sto dando io, è un problema?}.

\begin{definition}[NI-EX-ME-CO]
    The \textbf{NI-EX-ME-CO} version of the algorithm is defined by the following equations:

    \begin{equation}
        \forall u, v \in V(G) \quad w_{uv}^- := w_{uv}^-(\mathrm e)
    \end{equation}

    \begin{equation}
        \forall w_{uv}^+ := w_1 w_{uv}^+(\mathrm c) + w_2 w_{uv}^+(\mathrm n) + w_3 w_{uv}^+(\mathrm x)
    \end{equation}

    where $w_1, w_2, w_3 \ge 0$ and $w_1 + w_2 + w_3 = 1$.
\end{definition}

\subsection{The clustering ILP}

The classical formulation of correlation clustering does not impose any restrictions on cluster sizes. However, all known driver identification methods inherently include cluster size limits, as these sizes directly affect the computational complexity of the algorithms \todo{portano l'esempio di un altro algoritmo chiamato CoMEt che oltre size 10-12 "fails to operate", ha senso inserirlo? forse si}. Therefore, \textcite{c3} introduce a cluster size constraint by assuming that all clusters are of size $k$ at most; clearly, setting $k$ equal to the total number of vertices effectively removes this constraint, allowing flexibility in cluster size selection.

Another reason for imposing a cluster size limit is the expectation that driver genes of specific cancer types will be grouped together, and recent findings indicate that only a small number of drivers are typically present in any given cancer type \todo{non menzionano nessuna fonte dalla quale tirano fuori questa informazione, ritrovatela da solo e aggiungi qualcosa}. If clusters are too large, they may include drivers from multiple cancer types, hiding the detailed separation of the drivers. Furthermore, introducing cluster size constraints helps to avoid the limitations of many clustering algorithms that often produce non-informative \curlyquotes{giant clusters} or singleton clusters \todo{di nuovo, non ci sono citazioni o esempi menzionati, ma vorrei mettecene}.

\begin{definition}[C3's ILP] \label{c3_ilp}
    The \textbf{C3 algorithm} can be defined by the following ILP:

    \begin{equation} \label{c3_first}
        \mathrm{minimize} \sum_{e \in E(G)} (w_e^+x_e + w_e^-(1 - x_e)),
    \end{equation}

    \begin{equation} \label{c3_second}
        \mathrm{subject \ to \ } x_{uv} \le x_{uz} + x_{zv}, \ u, v, z \in V(G) \mathrm{\ distinct},
    \end{equation}

    \begin{equation} \label{c3_third}
        \sum_{v \in V(G) \atop u \neq v} (1 - x_{uv}) \le k, \ u \in V(G),
    \end{equation}

    \begin{equation} \label{c3_fourth}
        x_e \in \{0, 1\}, \ e \in E(G).
    \end{equation}
\end{definition}

In this formulation, $x_e$ allow to describe any clustering of the vertices of $G$, since $x_e \in \{0, 1\}$; also, note that \cref{c3_first} aligns with the definition provided in \cref{c3_chap2}, in fact $x_{uv} = 1$ implies that $u$ and $v$ should belong to different clusters, while $x_{uv} = 0$ implies that the two vertices should be placed into the same cluster. Furthermore, \cref{c3_third} states that for a fixed vertex $u \in V(G)$, the number of variables $x_{uv}$ equal to 0 --- for any $v \in V(G)$ such that $u \neq v$, meaning that $u$ and $v$ are adjacent and in the same cluster --- must not exceed $k$. Lastly, \cref{c3_second} is the \href{https://en.wikipedia.org/wiki/Triangle_inequality}{triangle inequality}, which ensures that if $u$ and $z$ are placed in the same cluster, and $z$ and $v$ are also placed in the same cluster, then $u$ and $v$ will be placed in the same clustered. This means that belonging to the same cluster is a transitive property \todo{dopo aver scritto questo mi sono fatto una domanda alla quale non trovo risposta nel paper: i cluster si possono intersecare? io non direi altrimenti questa definizione sottoforma di ILP non credo potrebbe avere senso, giusto?}, since $$\soe{l}{x_{uz} = 0 \\ x_{zv} = 0 \\ x_{uv} \le x_{uz} + x_{zv}} \implies x_{uv} = 0$$

\subsection{The rounding procedure}

Since solving ILPs is NP-Hard, \textcite{c3} relax the problem by changing \cref{c3_fourth} to an interval constraint $$0 \le x_e \le 1$$ leading to an LP program, the solution of which may be fractional. Hence, to obtain a valid clustering, the fractional solutions have to be rounded. Therefore, instead of solving the LP, \textcite{c3} remove \cref{c3_third} from the linear program, and employ the following rounding procedure.

\begin{algorithm}[H]
    \caption{
        \textit{Rounding procedure}: given a solution $\{x_e\}_{e \in E(G)}$ of \cref{c3_ilp} (without the size constraint \cref{c3_third}), a rational value $\alpha$, and an integer $k$, the algorithm rounds the solution to integer values.
    }

        \label{rounding_procedure}
    \begin{algorithmic}[1]
        \Function{roundingProcedure}{$G$, $\{x_e\}_{e \in E(G)}$, $\alpha$, $k$}
            \State $\mathcal C := \varnothing$ \Comment{the output set of clusters}
            \State $S := V(G)$
            \While{$S \neq \varnothing$}
                \State Choose an arbitrary $u \in S$ \Comment{this is the \textit{pivot vertex}}
                \State $T := \{w \in S - \{u\} \mid x_{uw} \le \alpha\}$ \Comment{$u$'s neighbors under $\alpha$'s threshold}
                \If{$\sum_{w \in T}{x_{uw}} \ge \frac{\alpha}{2}\abs{T}$}
                    \State $\mathcal C = \mathcal C \cup \{u\}$ \Comment{add a singleton cluster $\{u\}$}
                    \State $S = S - \{u\}$
                \ElsIf{$\abs{T} \le k$}
                    \State $\mathcal C = \mathcal C \cup \rbk{\{u\} \cup T}$ \Comment{add the cluster $(\{u\} \cup T)$}
                    \State $S = S - \rbk{\{u\} \cup T}$
                \Else
            \State Partition $T$ into $\{T_0', T_1, \ldots, T_p\}$, such that: \begin{itemize} \item[] \quad \quad \quad \quad $\sbullet \ \abs{T_0'} = k$ \item[] \quad \quad \quad \quad $\sbullet \ \abs{T_i} = k + 1$ for each $0 < i < p$ \item[] \quad \quad \quad \quad $\sbullet \ \abs{T_p} \le k + 1$ \end{itemize}
                    \State $T_0 := T_0' \cup \{u\}$ \Comment{$T_0$ has $k + 1$ elements}
                    \For{$i \in [0, p]$}
                        \State $\mathcal C = \mathcal C \cup T_i$ \Comment{add each partition as a cluster}
                    \EndFor
                    \State $S = S - \rbk{\{u\} \cup T}$
                \EndIf
            \EndWhile
            \State \textbf{return} $\mathcal C$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

placeholder. \todo{volevo dare una spiegazione a parole dell'algoritmo ma dopo averlo scritto mi sono reso conto che è abbastanza autoesplicativo, l'unica cosa che è un po vaga è la condizione della riga 7 che non comprendo del tutto}

placeholder. \todo{nel materiale supplementare trattano di un valore ottimale per $\alpha$, lo vedo?}

% The algorithm starts by defining an empty set $\mathcal C$, which will contain the clusters of $G$, and a set $S = V(G)$, which will be used to partition $V(G)$. Until $S$ is not empty, repeat the following:
%
% \begin{itemize}
%     \item choose an arbitrary vertex $u \in S$, which will be referred to as the \textit{pivot vertex};
%     \item define $T$ to be the set of the pivot's neighbors $w$ such that $x_{uw}$ is less than the given threshold $\alpha$;
%     \item if the sum of the elements of $T$ is more than half of the elements comprising $T$, scaled by $\alpha$ \todo{non capisco perché moltiplicano per $\alpha$}, then add a singleton cluster $\{u\}$ to $\mathcal C$ and remove the pivot from $S$;
%     \item otherwise, if the number of filtered neighbors of the pivot is at most $k$, create a $\{u\} \cup T$ cluster and remove every one of its vertices from $S$;
%     \item if none of the previous conditions were met, then $T$ must be partitioned into $k + 1$ sized clusters, one of which must contain the pivot (and there may be a set with fewer vertices).
% \end{itemize}

\begin{definition}[C3] \label{c3_final}
    The \textbf{C3 algorithm} is defined as follows: first, the next ILP is solved

    \begin{equation}
        \mathrm{minimize} \sum_{e \in E(G)} (w_e^+x_e + w_e^-(1 - x_e)),
    \end{equation}

    \begin{equation}
        \mathrm{subject \ to \ } x_{uv} \le x_{uz} + x_{zv}, \ u, v, z \in V(G) \mathrm{\ distinct},
    \end{equation}

    \begin{equation}
        0 \le x_e \le 1, \ e \in E(G).
    \end{equation}

    and then the rounding procedure defined in \cref{rounding_procedure} is applied.

\end{definition}

\cleardoublepage
